"""
LLM Client - LLM 客户端封装
"""
from typing import Dict, Any, Optional, List
import json
import requests


class LLMClient:
    """
    LLM 客户端，支持多种 LLM 提供商
    """

    def __init__(self, provider: str = 'openai',
                 api_key: str = None,
                 model: str = 'gpt-4',
                 base_url: str = None,
                 temperature: float = 0.7,
                 max_tokens: int = 2000):
        """
        初始化 LLM 客户端

        Args:
            provider: LLM 提供商 ('openai', 'anthropic', 'ollama', 'custom')
            api_key: API 密钥
            model: 模型名称
            base_url: 自定义 API 端点
            temperature: 温度参数
            max_tokens: 最大 token 数
        """
        self.provider = provider
        self.api_key = api_key
        self.model = model
        self.base_url = base_url
        self.temperature = temperature
        self.max_tokens = max_tokens

        # 根据提供商设置默认端点
        if provider == 'openai' and not base_url:
            self.base_url = 'https://api.openai.com/v1'
        elif provider == 'anthropic' and not base_url:
            self.base_url = 'https://api.anthropic.com/v1'
        elif provider == 'ollama' and not base_url:
            self.base_url = 'http://localhost:11434/v1'

    def generate(self, prompt: str,
                system_prompt: Optional[str] = None,
                temperature: Optional[float] = None,
                max_tokens: Optional[int] = None) -> str:
        """
        生成文本

        Args:
            prompt: 用户提示
            system_prompt: 系统提示（可选）
            temperature: 温度参数（覆盖默认值）
            max_tokens: 最大 token 数（覆盖默认值）

        Returns:
            生成的文本
        """
        temp = temperature or self.temperature
        tokens = max_tokens or self.max_tokens

        if self.provider in ['openai', 'ollama', 'custom']:
            return self._generate_openai_compatible(
                prompt, system_prompt, temp, tokens
            )
        elif self.provider == 'anthropic':
            return self._generate_anthropic(
                prompt, system_prompt, temp, tokens
            )
        else:
            raise ValueError(f"Unsupported provider: {self.provider}")

    def _generate_openai_compatible(self, prompt: str,
                                    system_prompt: Optional[str],
                                    temperature: float,
                                    max_tokens: int) -> str:
        """使用 OpenAI 兼容的 API 生成文本"""
        messages = []

        if system_prompt:
            messages.append({'role': 'system', 'content': system_prompt})

        messages.append({'role': 'user', 'content': prompt})

        headers = {
            'Content-Type': 'application/json'
        }

        if self.api_key:
            headers['Authorization'] = f'Bearer {self.api_key}'

        data = {
            'model': self.model,
            'messages': messages,
            'temperature': temperature,
            'max_tokens': max_tokens
        }

        response = requests.post(
            f'{self.base_url}/chat/completions',
            headers=headers,
            json=data,
            timeout=60
        )

        response.raise_for_status()
        result = response.json()

        return result['choices'][0]['message']['content']

    def _generate_anthropic(self, prompt: str,
                           system_prompt: Optional[str],
                           temperature: float,
                           max_tokens: int) -> str:
        """使用 Anthropic API 生成文本"""
        headers = {
            'Content-Type': 'application/json',
            'x-api-key': self.api_key,
            'anthropic-version': '2023-06-01'
        }

        data = {
            'model': self.model,
            'max_tokens': max_tokens,
            'temperature': temperature,
            'messages': [{'role': 'user', 'content': prompt}]
        }

        if system_prompt:
            data['system'] = system_prompt

        response = requests.post(
            f'{self.base_url}/messages',
            headers=headers,
            json=data,
            timeout=60
        )

        response.raise_for_status()
        result = response.json()

        return result['content'][0]['text']

    def analyze_vulnerability(self, request_data: Dict[str, Any],
                             response_data: Dict[str, Any],
                             context: Optional[Dict] = None) -> Dict[str, Any]:
        """
        分析请求响应对是否存在漏洞

        Args:
            request_data: 请求数据
            response_data: 响应数据
            context: 额外的上下文信息

        Returns:
            分析结果字典
        """
        from .prompt_templates import PromptTemplates

        prompt = PromptTemplates.vulnerability_analysis_prompt(
            request_data, response_data, context
        )

        system_prompt = PromptTemplates.get_system_prompt()

        response = self.generate(prompt, system_prompt)

        # 尝试解析为 JSON
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            # 如果不是 JSON，返回原始文本
            return {'analysis': response}

    def suggest_payloads(self, endpoint: str, parameter: str,
                         vuln_type: str,
                         context: Optional[Dict] = None) -> List[str]:
        """
        为特定漏洞类型建议测试 Payload

        Args:
            endpoint: 目标端点
            parameter: 参数名
            vuln_type: 漏洞类型
            context: 额外的上下文信息

        Returns:
            Payload 列表
        """
        from .prompt_templates import PromptTemplates

        prompt = PromptTemplates.payload_suggestion_prompt(
            endpoint, parameter, vuln_type, context
        )

        system_prompt = PromptTemplates.get_system_prompt()

        response = self.generate(prompt, system_prompt)

        # 尝试解析为 JSON
        try:
            result = json.loads(response)
            return result.get('payloads', [])
        except json.JSONDecodeError:
            # 如果解析失败，尝试按行分割
            return [line.strip() for line in response.split('\n')
                   if line.strip() and not line.startswith('#')]

    def explain_response(self, response_body: str,
                        vuln_type: str) -> str:
        """
        解释响应内容是否表明存在漏洞

        Args:
            response_body: 响应体内容
            vuln_type: 漏洞类型

        Returns:
            解释文本
        """
        from .prompt_templates import PromptTemplates

        prompt = PromptTemplates.response_explanation_prompt(
            response_body, vuln_type
        )

        system_prompt = PromptTemplates.get_system_prompt()

        return self.generate(prompt, system_prompt)
